{
  "id": "harrywang/housing",
  "id_no": 24824,
  "datasetSlugNullable": "housing",
  "ownerUserNullable": "harrywang",
  "usabilityRatingNullable": 1.0,
  "titleNullable": "California Housing Data (1990)",
  "subtitleNullable": "California Housing Price Prediction",
  "descriptionNullable": "# Source\n\nThis is the dataset used in this book: https://github.com/ageron/handson-ml/tree/master/datasets/housing to illustrate a sample end-to-end ML project workflow (pipeline). This is a great book - I highly recommend!\n\nThe data is based on California Census in 1990.\n\n### About the Data (from the book):\n\n\"This dataset is a modified version of the California Housing dataset available from Lu\u00eds Torgo's page (University of Porto). Lu\u00eds Torgo obtained it from the StatLib repository (which is closed now). The dataset may also be downloaded from StatLib mirrors.\n\nThe following is the description from the book author:\n\nThis dataset appeared in a 1997 paper titled Sparse Spatial Autoregressions by Pace, R. Kelley and Ronald Barry, published in the Statistics and Probability Letters journal. They built it using the 1990 California census data. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n\nThe dataset in this directory is almost identical to the original, with two differences: \n207 values were randomly removed from the total_bedrooms column, so we can discuss what to do with missing data.\nAn additional categorical attribute called ocean_proximity was added, indicating (very roughly) whether each block group is near the ocean, near the Bay area, inland or on an island. This allows discussing what to do with categorical data.\nNote that the block groups are called \"districts\" in the Jupyter notebooks, simply because in some contexts the name \"block group\" was confusing.\"\n\n### About the Data (From Lu\u00eds Torgo page): \nhttp://www.dcc.fc.up.pt/%7Eltorgo/Regression/cal_housing.html\n\nThis is a dataset obtained from the StatLib repository. Here is the included description:\n\n\"We collected information on the variables using all the block groups in California from the 1990 Cens us. In this sample a block group on average includes 1425.5 individuals living in a geographically co mpact area. Naturally, the geographical area included varies inversely with the population density. W e computed distances among the centroids of each block group as measured in latitude and longitude. W e excluded all the block groups reporting zero entries for the independent and dependent variables. T he final data contained 20,640 observations on 9 variables. The dependent variable is ln(median house value).\"\n\n\n### End-to-End ML Project Steps (Chapter 2 of the book)\n\n1. Look at the big picture \n2. Get the data\n3. Discover and visualize the data to gain insights\n4. Prepare the data for Machine Learning algorithms\n5. Select a model and train it\n6. Fine-tune your model \n7. Present your solution\n8. Launch, monitor, and maintain your system\n\n# The 10-Step Machine Learning Project Workflow (My Version)\n\n1. Define business object\n2. Make sense of the data from a high level\n    - data types (number, text, object, etc.)\n    - continuous/discrete\n    - basic stats (min, max, std, median, etc.) using boxplot\n    - frequency via histogram\n    - scales and distributions of different features\n3. Create the traning and test sets using proper sampling methods, e.g., random vs. stratified\n4. Correlation analysis (pair-wise and attribute combinations)\n5. Data cleaning (missing data, outliers, data errors)\n6. Data transformation via pipelines (categorical text to number using one hot encoding, feature scaling via normalization/standardization, feature combinations)\n7. Train and cross validate different models and select the most promising one (Linear Regression, Decision Tree, and Random Forest were tried in this tutorial)\n8. Fine tune the model using trying different combinations of hyperparameters\n9. Evaluate the model with best estimators in the test set\n10. Launch, monitor, and refresh the model and system\n",
  "datasetId": 24824,
  "datasetSlug": "housing",
  "hasDatasetSlug": true,
  "ownerUser": "harrywang",
  "hasOwnerUser": true,
  "usabilityRating": 1.0,
  "hasUsabilityRating": true,
  "totalViews": 107077,
  "totalVotes": 153,
  "totalDownloads": 14319,
  "title": "California Housing Data (1990)",
  "hasTitle": true,
  "subtitle": "California Housing Price Prediction",
  "hasSubtitle": true,
  "description": "# Source\n\nThis is the dataset used in this book: https://github.com/ageron/handson-ml/tree/master/datasets/housing to illustrate a sample end-to-end ML project workflow (pipeline). This is a great book - I highly recommend!\n\nThe data is based on California Census in 1990.\n\n### About the Data (from the book):\n\n\"This dataset is a modified version of the California Housing dataset available from Lu\u00eds Torgo's page (University of Porto). Lu\u00eds Torgo obtained it from the StatLib repository (which is closed now). The dataset may also be downloaded from StatLib mirrors.\n\nThe following is the description from the book author:\n\nThis dataset appeared in a 1997 paper titled Sparse Spatial Autoregressions by Pace, R. Kelley and Ronald Barry, published in the Statistics and Probability Letters journal. They built it using the 1990 California census data. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n\nThe dataset in this directory is almost identical to the original, with two differences: \n207 values were randomly removed from the total_bedrooms column, so we can discuss what to do with missing data.\nAn additional categorical attribute called ocean_proximity was added, indicating (very roughly) whether each block group is near the ocean, near the Bay area, inland or on an island. This allows discussing what to do with categorical data.\nNote that the block groups are called \"districts\" in the Jupyter notebooks, simply because in some contexts the name \"block group\" was confusing.\"\n\n### About the Data (From Lu\u00eds Torgo page): \nhttp://www.dcc.fc.up.pt/%7Eltorgo/Regression/cal_housing.html\n\nThis is a dataset obtained from the StatLib repository. Here is the included description:\n\n\"We collected information on the variables using all the block groups in California from the 1990 Cens us. In this sample a block group on average includes 1425.5 individuals living in a geographically co mpact area. Naturally, the geographical area included varies inversely with the population density. W e computed distances among the centroids of each block group as measured in latitude and longitude. W e excluded all the block groups reporting zero entries for the independent and dependent variables. T he final data contained 20,640 observations on 9 variables. The dependent variable is ln(median house value).\"\n\n\n### End-to-End ML Project Steps (Chapter 2 of the book)\n\n1. Look at the big picture \n2. Get the data\n3. Discover and visualize the data to gain insights\n4. Prepare the data for Machine Learning algorithms\n5. Select a model and train it\n6. Fine-tune your model \n7. Present your solution\n8. Launch, monitor, and maintain your system\n\n# The 10-Step Machine Learning Project Workflow (My Version)\n\n1. Define business object\n2. Make sense of the data from a high level\n    - data types (number, text, object, etc.)\n    - continuous/discrete\n    - basic stats (min, max, std, median, etc.) using boxplot\n    - frequency via histogram\n    - scales and distributions of different features\n3. Create the traning and test sets using proper sampling methods, e.g., random vs. stratified\n4. Correlation analysis (pair-wise and attribute combinations)\n5. Data cleaning (missing data, outliers, data errors)\n6. Data transformation via pipelines (categorical text to number using one hot encoding, feature scaling via normalization/standardization, feature combinations)\n7. Train and cross validate different models and select the most promising one (Linear Regression, Decision Tree, and Random Forest were tried in this tutorial)\n8. Fine tune the model using trying different combinations of hyperparameters\n9. Evaluate the model with best estimators in the test set\n10. Launch, monitor, and refresh the model and system\n",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "housing",
    "regression"
  ],
  "licenses": [
    {
      "nameNullable": "copyright-authors",
      "name": "copyright-authors",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}